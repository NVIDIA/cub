<!-- HTML header for doxygen 1.8.3.1-->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.3.1"/>
<title>CUB: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="extra_stylesheet.css" rel="stylesheet" type="text/css"/>
<link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-38890655-1']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">CUB
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.3.1 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Namespaces</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(5)"><span class="SelectionMark">&#160;</span>Variables</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(6)"><span class="SelectionMark">&#160;</span>Typedefs</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(7)"><span class="SelectionMark">&#160;</span>Enumerations</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(8)"><span class="SelectionMark">&#160;</span>Enumerator</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(9)"><span class="SelectionMark">&#160;</span>Groups</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(10)"><span class="SelectionMark">&#160;</span>Pages</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">CUB Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="toc"><h3>Table of Contents</h3>
<ul><li class="level1"><a href="#sec0">(1) What is CUB?</a></li>
<li class="level1"><a href="#sec2">(2) Recent news</a></li>
<li class="level1"><a href="#sec3">(3) A simple example</a></li>
<li class="level1"><a href="#sec4">(4) Why do you need CUB?</a></li>
<li class="level1"><a href="#sec5">(5) Where is CUB positioned in the CUDA ecosystem?</a></li>
<li class="level1"><a href="#sec6">(6) How does CUB work?</a><ul><li class="level2"><a href="#sec3sec1">6.1 &nbsp;&nbsp; C++ templates</a></li>
<li class="level2"><a href="#sec3sec2">6.2 &nbsp;&nbsp; Reflective type structure</a></li>
<li class="level2"><a href="#sec3sec3">6.3 &nbsp;&nbsp; Flexible mapping of data onto threads</a></li>
</ul>
</li>
<li class="level1"><a href="#sec7">(7) Contributors</a></li>
<li class="level1"><a href="#sec8">(8) Open Source License</a></li>
</ul>
</div>
<div class="textblock"> 
<a href="http://research.nvidia.com"><img src="nvresearch.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="http://research.nvidia.com"><em>NVIDIA Research</em></a>
<br>
<a href="https://github.com/NVlabs/cub"><img src="github-icon-747d8b799a48162434b2c0595ba1317e.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="https://github.com/NVlabs/cub"><em>Browse or fork CUB at GitHub!</em></a>
<br>
<a href="http://groups.google.com/group/cub-users"><img src="groups-icon.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="http://groups.google.com/group/cub-users"><em>Join the cub-users discussion forum!</em></a>
<br>
<a href="download_cub.html"><img src="download-icon.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="download_cub.html"><em>Download CUB!</em></a>
<h1><a class="anchor" id="sec0"></a>
(1) What is CUB?</h1>
<dl class="section user"><dt></dt><dd><b><em>Overview</em></b>. CUB is an open-source library of high performance primitives and utilities for constructing CUDA software. CUB provides reusable software components for every layer of execution within the CUDA programming model:<ul>
<li><a href="group___device_module.html"><b><em>Device-wide primitives</em></b></a><ul>
<li>Global histogram, reduction, etc. (More coming soon..)</li>
<li>Compatible with CUDA dynamic/nested parallelism</li>
</ul>
</li>
<li><a href="group___block_module.html"><b><em>Block-wide primitives</em></b></a><ul>
<li>Local radix sort, prefix scan, histogram, reduction, I/O, etc.</li>
<li>Compatible with arbitrary thread block sizes</li>
</ul>
</li>
<li><a href="group___warp_module.html"><b><em>Warp-wide primitives</em></b></a><ul>
<li>Local prefix scan, reduction, etc.</li>
</ul>
</li>
<li><a href="group___thread_module.html"><b><em>Thread</em></b></a>, <a href="group___grid_module.html"><b><em>grid dispatch</em></b></a>, and <a href="group___thread_module.html"><b><em>resource utilities</em></b></a><ul>
<li>PTX intrinsics, device reflection, work distribution, caching memory allocators, etc.</li>
</ul>
</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd><b><em>Motivation</em></b>. CUB is inspired by the following goals:<ul>
<li><em>Absolute performance</em>. CUB primitives are specialized and tuned to best match the features and capabilities of every CUDA architecture.</li>
<li><em>Enhanced programmer productivity</em>. CUB primitives allow developers to quickly string together sequences of complex parallel operations at all levels of execution.</li>
<li><em>Reduced maintenance burden</em>. CUB provides layers of abstraction over the complexities of diverse CUDA hardware. With CUB, applications can enjoy performance-portability without the intensive rewriting or porting efforts typically needed to accommodate new device instructions or behavior.</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd><b><em>Collective primitives</em></b>. CUB is unique in that it provides cooperative block-level and warp-level parallel primitives. Unlike traditional library software components, these operations are <em>collective</em>, i.e., they are called by a group of parallel SIMT threads rather than by a single sequential thread. These collective primitives are not bound to any particular width of parallelism or to any particular data type. This allows them to be flexible and tunable to fit the needs of the enclosing kernel computation. </dd></dl>
<dl class="section user"><dt></dt><dd>Thus CUB is <a href="index.html"><em>CUDA Unbound</em></a>.</dd></dl>
<dl class="section user"><dt></dt><dd><div class="image">
<img src="cub_overview.png" alt="cub_overview.png"/>
</div>
 <div class="centercaption">Orientation of <em>collective</em> CUB thread block primitives within the CUDA software stack</div></dd></dl>
<h1><a class="anchor" id="sec2"></a>
(2) Recent news</h1>
<dl class="section user"><dt></dt><dd><table class="doxtable">
<tr>
<td>04/30/2013 </td><td style="white-space: nowrap"><a href="https://github.com/NVlabs/cub/archive/0.9.3.zip">CUB v0.9.3 (update release)</a> </td><td>Introduced several new device-wide and block-wide primitives, including 256-bin histogram. Misc. cosmetic and bug fixes. See the <a href="https://github.com/NVlabs/cub/blob/master/CHANGE_LOG.TXT">change-log</a> for further details.  </td></tr>
<tr>
<td>04/04/2013 </td><td style="white-space: nowrap"><a href="https://github.com/NVlabs/cub/archive/0.9.2.zip">CUB v0.9.2 (update release)</a> </td><td>Minor cosmetic, feature, and compilation updates. See the <a href="https://github.com/NVlabs/cub/blob/master/CHANGE_LOG.TXT">change-log</a> for further details.  </td></tr>
<tr>
<td>03/07/2013 </td><td style="white-space: nowrap"><a href="https://github.com/NVlabs/cub/archive/0.9.zip">CUB v0.9 ("preview" release)</a> </td><td>CUB is the first durable, high-performance library of cooperative threadblock, warp, and thread primitives for CUDA kernel programming. More primitives and examples coming soon!  </td></tr>
</table>
</dd></dl>
<h1><a class="anchor" id="sec3"></a>
(3) A simple example</h1>
<dl class="section user"><dt></dt><dd>The following code snippet illustrates a simple CUDA kernel for sorting a thread block's data:</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line"><span class="preprocessor">#include &lt;<a class="code" href="cub_8cuh.html">cub/cub.cuh</a>&gt;</span></div>
<div class="line"></div>
<div class="line"><span class="comment">// An tile-sorting CUDA kernel</span></div>
<div class="line"><span class="keyword">template</span> &lt;</div>
<div class="line">     <span class="keywordtype">int</span>         BLOCK_THREADS,              <span class="comment">// Threads per block</span></div>
<div class="line">     <span class="keywordtype">int</span>         ITEMS_PER_THREAD,           <span class="comment">// Items per thread</span></div>
<div class="line">     <span class="keyword">typename</span>    T&gt;                          <span class="comment">// Numeric data type</span></div>
<div class="line">__global__ <span class="keywordtype">void</span> TileSortKernel(T *d_in, T *d_out)</div>
<div class="line">{</div>
<div class="line">     <span class="keyword">using namespace </span>cub;</div>
<div class="line">     <span class="keyword">const</span> <span class="keywordtype">int</span> TILE_SIZE = BLOCK_THREADS * ITEMS_PER_THREAD;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Parameterize cub::BlockRadixSort for the parallel execution context</span></div>
<div class="line">     <span class="keyword">typedef</span> <a class="code" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">BlockRadixSort&lt;T, BLOCK_THREADS&gt;</a> BlockRadixSort;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Declare the shared memory needed by BlockRadixSort</span></div>
<div class="line">     __shared__ <span class="keyword">typename</span> <a class="code" href="classcub_1_1_block_radix_sort.html#ab42b81b7a6587ecaca18f232104b3f3c" title="The operations exposed by BlockRadixSort require shared memory of this type. This opaque storage can ...">BlockRadixSort::SmemStorage</a> smem_storage;</div>
<div class="line"></div>
<div class="line">     <span class="comment">// A segment of data items per thread</span></div>
<div class="line">     T data[ITEMS_PER_THREAD];</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Load a tile of data using vector-load instructions</span></div>
<div class="line">     <a class="code" href="group___block_module.html#gaea8200ef976bb588c569e039ea79005c" title="Load a tile of items across a threadblock directly using the specified cache modifier.">BlockLoadVectorized</a>(data, d_in + (blockIdx.x * TILE_SIZE));</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Sort data in ascending order</span></div>
<div class="line">     <a class="code" href="classcub_1_1_block_radix_sort.html#a779bfcd00c57f6b97cbbb8a0aafb616a" title="Performs a threadblock-wide radix sort over a blocked arrangement of keys.">BlockRadixSort::SortBlocked</a>(smem_storage, data);</div>
<div class="line"></div>
<div class="line">     <span class="comment">// Store the sorted tile using vector-store instructions</span></div>
<div class="line">     <a class="code" href="group___block_module.html#ga013c3ab8214854f45e8d678958e7dde9" title="Store a tile of items across a threadblock directly using the specified cache modifier.">BlockStoreVectorized</a>(data, d_out + (blockIdx.x * TILE_SIZE));</div>
<div class="line">}</div>
</div><!-- fragment --></dd></dl>
<dl class="section user"><dt></dt><dd>The <a class="el" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">cub::BlockRadixSort</a> type performs a cooperative radix sort across the thread block's data items. Its implementation is parameterized by the number of threads per block and the aggregate data type <code>T</code> and is specialized for the underlying architecture.</dd></dl>
<dl class="section user"><dt></dt><dd>Once instantiated, the <a class="el" href="classcub_1_1_block_radix_sort.html" title="BlockRadixSort provides variants of parallel radix sorting across a CUDA threadblock. .">cub::BlockRadixSort</a> type exposes an opaque <a class="el" href="classcub_1_1_block_radix_sort.html#ab42b81b7a6587ecaca18f232104b3f3c" title="The operations exposed by BlockRadixSort require shared memory of this type. This opaque storage can ...">cub::BlockRadixSort::SmemStorage</a> member type. The thread block uses this storage type to allocate the shared memory needed by the primitive. This storage type can be aliased or <code>union</code>'d with other types so that the shared memory can be reused for other purposes.</dd></dl>
<dl class="section user"><dt></dt><dd>Furthermore, the kernel uses CUB's primitives for vectorizing global loads and stores. For example, lower-level <code>ld.global.v4.s32</code> <a href="http://docs.nvidia.com/cuda/parallel-thread-execution">PTX instructions</a> will be generated when <code>T</code> = <code>int</code> and <code>ITEMS_PER_THREAD</code> is a multiple of 4.</dd></dl>
<h1><a class="anchor" id="sec4"></a>
(4) Why do you need CUB?</h1>
<dl class="section user"><dt></dt><dd>CUDA kernel software is where the complexity of parallelism is expressed. Programmers must reason about deadlock, livelock, synchronization, race conditions, shared memory layout, plurality of state, granularity, throughput, latency, memory bottlenecks, etc. Constructing and fine-tuning kernel code is perhaps the most challenging, time-consuming aspect of CUDA programming.</dd></dl>
<dl class="section user"><dt></dt><dd>However, with the exception of CUB, there are few (if any) software libraries of reusable kernel primitives. In the CUDA ecosystem, CUB is unique in this regard. As a <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation">SIMT</a> library and software abstraction layer, CUB provides:<ol type="1">
<li><b>Simplicity of composition.</b> Parallel CUB primitives can be simply sequenced together in kernel code. (This convenience is analogous to programming with <a href="http://thrust.github.com/"><b><em>Thrust</em></b></a> primitives in the host program.)</li>
<li><b>High performance.</b> CUB simplifies high performance kernel development by taking care to implement and make available the fastest available algorithms, strategies, and techniques.</li>
<li><b>Performance portability.</b> CUB primitives are specialized to match the target hardware. Furthermore, the CUB library continually evolves to accommodate new algorithmic developments, hardware instructions, etc.</li>
<li><b>Simplicity of performance tuning.</b> CUB primitives provide parallel abstractions whose performance behavior can be statically tuned. For example, most CUB primitives support alternative algorithmic strategies and variable grain sizes (threads per block, items per thread, etc.).</li>
<li><b>Robustness and durability.</b> CUB primitives are designed to function properly for arbitrary data types and widths of parallelism (not just for the built-in C++ types or for powers-of-two threads per block).</li>
</ol>
</dd></dl>
<h1><a class="anchor" id="sec5"></a>
(5) Where is CUB positioned in the CUDA ecosystem?</h1>
<dl class="section user"><dt></dt><dd>The CUDA ecosystem engenders four different layers of software abstraction:</dd></dl>
<table  border="0px" cellpadding="0px" cellspacing="0px">
<tr>
<td width="50%"><dl class="section user"><dt></dt><dd><b>Runtime-agnostic</b>. A single thread calls through a library interface to perform some data-parallel function. The use of any particular parallel or sequential computing backend (e.g., CUDA, TBB, OpenMP, etc.) is opaque to the caller. Although these libraries are not CUDA-specific, primitives such as those provided by CUB can be used to implement them. Examples include:<ul>
<li><a href="http://thrust.github.com/"><b><em>Thrust</em></b></a></li>
<li><a href="https://github.com/harrism/hemi"><b><em>Hemi</em></b></a></li>
<li><a href="http://www.accelereyes.com/products/arrayfire"><b><em>ArrayFire</em></b></a>  </li>
</ul>
</dd></dl>
</td><td width="50%"> 
<a href="generic_abstraction.png"><img src="generic_abstraction.png" width="100%" border="0px"/></a>
  </td></tr>
<tr>
<td width="50%"><dl class="section user"><dt></dt><dd><b>CUDA kernel</b>. A single CPU thread invokes a CUDA kernel to perform some data-parallel function. The incorporation of entire kernels (and their corresponding invocation stubs) into libraries is the most common form of code reuse for CUDA. Libraries of CUDA kernels include the following:<ul>
<li><a href="https://developer.nvidia.com/cublas"><b><em>cuBLAS</em></b></a></li>
<li><a href="https://developer.nvidia.com/cufft"><b><em>cuFF</em>T</b></a></li>
<li><a href="https://developer.nvidia.com/cusparse"><b><em>cuSPARSE</em></b></a></li>
<li><a href="index.html"><b><em>CUB</em></b></a>  </li>
</ul>
</dd></dl>
</td><td width="50%"> 
<a href="kernel_abstraction.png"><img src="kernel_abstraction.png" width="100%" border="0px"/></a>
  </td></tr>
<tr>
<td><dl class="section user"><dt></dt><dd><b>Thread blocks (SIMT)</b>. Each kernel invocation comprises some number of parallel threads. Threads are grouped into blocks, and the entire block of threads invokes some cooperative function in which they communicate and synchronize with each other. There has historically been very little reuse of cooperative SIMT software within CUDA kernel. Libraries of thread-block primitives include the following:<ul>
<li><a href="index.html"><b><em>CUB</em></b></a>  </li>
</ul>
</dd></dl>
</td><td> 
<a href="simt_abstraction.png"><img src="simt_abstraction.png" width="100%" border="0px"/></a>
  </td></tr>
<tr>
<td><dl class="section user"><dt></dt><dd><b>CUDA thread</b>. A single CUDA thread invokes some sequential function. This is the finest-grained level of CUDA software abstraction and requires no consideration for the scheduling or synchronization of parallel threads. CUDA libraries of purely data-parallel functions include the following:<ul>
<li><a href="http://docs.nvidia.com/cuda/cuda-math-api/index.html"><b><em> CUDA Math</em></b></a>, <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-functions"><b><em>Texture</em></b></a>, and <a href="http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions"><b><em>Atomic</em></b></a> APIs</li>
<li><a href="https://developer.nvidia.com/curand"><b><em>cuRAND</em></b></a>'s device-code interface</li>
<li><a href="index.html"><b><em>CUB</em></b></a>  </li>
</ul>
</dd></dl>
</td><td> 
<a href="devfun_abstraction.png"><img src="devfun_abstraction.png" width="100%" border="0px"/></a>
  </td></tr>
</table>
<h1><a class="anchor" id="sec6"></a>
(6) How does CUB work?</h1>
<dl class="section user"><dt></dt><dd>CUB leverages the following programming idioms:<ol type="1">
<li><a href="index.html#sec3sec1"><b>C++ templates</b></a></li>
<li><a href="index.html#sec3sec2"><b>Reflective type structure</b></a></li>
<li><a href="index.html#sec3sec3"><b>Flexible data mapping</b></a></li>
</ol>
</dd></dl>
<h2><a class="anchor" id="sec3sec1"></a>
6.1    C++ templates</h2>
<dl class="section user"><dt></dt><dd>As a SIMT library, CUB must be flexible enough to accommodate a wide spectrum of <em>parallel execution contexts</em>, i.e., specific:<ul>
<li>Data types</li>
<li>Widths of parallelism (threads per block)</li>
<li>Grain sizes (data items per thread)</li>
<li>Underlying architectures (special instructions, warp size, rules for bank conflicts, etc.)</li>
<li>Tuning requirements (e.g., latency vs. throughput)</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd>To provide this flexibility, CUB is implemented as a C++ template library. C++ templates are a way to write generic algorithms and data structures. There is no need to build CUB separately. You simply <code>#include</code> the <code><a class="el" href="cub_8cuh.html">cub.cuh</a></code> header file into your <code>.cu</code> CUDA C++ sources and compile with NVIDIA's <code>nvcc</code> compiler.</dd></dl>
<h2><a class="anchor" id="sec3sec2"></a>
6.2    Reflective type structure</h2>
<dl class="section user"><dt></dt><dd>Cooperation within a thread block requires shared memory for communicating between threads. However, the specific size and layout of the memory needed by a given primitive will be specific to the details of its parallel execution context (e.g., how many threads are calling into it, how many items are processed per thread, etc.). Furthermore, this shared memory must be allocated outside of the component itself if it is to be reused elsewhere by the thread block.</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line"><span class="comment">// Parameterize a BlockScan type for use with 128 threads</span></div>
<div class="line"><span class="comment">// and 4 items per thread</span></div>
<div class="line"><span class="keyword">typedef</span> <a class="code" href="classcub_1_1_block_scan.html" title="BlockScan provides variants of parallel prefix scan (and prefix sum) across a CUDA threadblock...">cub::BlockScan&lt;unsigned int, 128, 4&gt;</a> BlockScan;</div>
<div class="line"></div>
<div class="line"><span class="comment">// Declare shared memory for BlockScan</span></div>
<div class="line">__shared__ <span class="keyword">typename</span> BlockScan::SmemStorage smem_storage;</div>
<div class="line"></div>
<div class="line"><span class="comment">// A segment of consecutive input items per thread</span></div>
<div class="line"><span class="keywordtype">int</span> data[4];</div>
<div class="line"></div>
<div class="line"><span class="comment">// Obtain data in blocked order</span></div>
<div class="line">...</div>
<div class="line"></div>
<div class="line"><span class="comment">// Perform an exclusive prefix sum across the tile of data</span></div>
<div class="line">BlockScan::ExclusiveSum(smem_storage, data, data);</div>
</div><!-- fragment --></dd></dl>
<dl class="section user"><dt></dt><dd>To address this issue, we encapsulate cooperative procedures within <em>reflective type structure</em> (C++ classes). As illustrated in the <a class="el" href="classcub_1_1_block_scan.html" title="BlockScan provides variants of parallel prefix scan (and prefix sum) across a CUDA threadblock...">cub::BlockScan</a> example above, these primitives are C++ classes with interfaces that expose both:<ul>
<li>Procedural entrypoints for a block of threads to invoke</li>
<li>An opaque shared memory type needed for the operation of those methods</li>
</ul>
</dd></dl>
<h2><a class="anchor" id="sec3sec3"></a>
6.3    Flexible mapping of data onto threads</h2>
<dl class="section user"><dt></dt><dd>We often design kernels such that each thread block is assigned a "tile" of data items for processing.</dd></dl>
<dl class="section user"><dt></dt><dd><div class="image">
<img src="tile.png" alt="tile.png"/>
</div>
 <div class="centercaption">Tile of eight ordered data items</div></dd></dl>
<dl class="section user"><dt></dt><dd>When the tile size equals the thread block size, the mapping of data onto threads is straightforward (one datum per thread). However, there are often performance advantages for processing more than one datum per thread. For these scenarios, CUB primitives will specify which of the following partitioning alternatives they accommodate:</dd></dl>
<table  border="0px" cellpadding="0px" cellspacing="0px">
<tr>
<td><dl class="section user"><dt></dt><dd><ul>
<li><b><em>Blocked arrangement</em></b>. The aggregate tile of items is partitioned evenly across threads in "blocked" fashion with thread<sub><em>i</em></sub> owning the <em>i</em><sup>th</sup> segment of consecutive elements. Blocked arrangements are often desirable for algorithmic benefits (where long sequences of items can be processed sequentially within each thread).  </li>
</ul>
</dd></dl>
</td><td><dl class="section user"><dt></dt><dd><div class="image">
<img src="blocked.png" alt="blocked.png"/>
</div>
 <div class="centercaption"><em>Blocked</em> arrangement across four threads <br/>
(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>  </dd></dl>
</td></tr>
<tr>
<td><dl class="section user"><dt></dt><dd><ul>
<li><b><em>Striped arrangement</em></b>. The aggregate tile of items is partitioned across threads in "striped" fashion, i.e., the <code>ITEMS_PER_THREAD</code> items owned by each thread have logical stride <code>BLOCK_THREADS</code> between them. Striped arrangements are often desirable for data movement through global memory (where <a href="http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-global-memory">read/write coalescing</a> is an important performance consideration).  </li>
</ul>
</dd></dl>
</td><td><dl class="section user"><dt></dt><dd><div class="image">
<img src="striped.png" alt="striped.png"/>
</div>
 <div class="centercaption"><em>Striped</em> arrangement across four threads <br/>
(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>  </dd></dl>
</td></tr>
</table>
<dl class="section user"><dt></dt><dd>The benefits of processing multiple items per thread (a.k.a., <em>register blocking</em>, <em>granularity coarsening</em>, etc.) include:<ul>
<li>Algorithmic efficiency. Sequential work over multiple items in thread-private registers is cheaper than synchronized, cooperative work through shared memory spaces.</li>
<li>Data occupancy. The number of items that can be resident on-chip in thread-private register storage is often greater than the number of schedulable threads.</li>
<li>Instruction-level parallelism. Multiple items per thread also facilitates greater ILP for improved throughput and utilization.</li>
</ul>
</dd></dl>
<dl class="section user"><dt></dt><dd>Finally, <a class="el" href="classcub_1_1_block_exchange.html" title="BlockExchange provides operations for reorganizing the partitioning of ordered data across a CUDA thr...">cub::BlockExchange</a> provides operations for converting between blocked and striped arrangements.</dd></dl>
<h1><a class="anchor" id="sec7"></a>
(7) Contributors</h1>
<dl class="section user"><dt></dt><dd>CUB is developed as an open-source project by <a href="http://research.nvidia.com">NVIDIA Research</a>. The primary contributor is <a href="http://github.com/dumerrill">Duane Merrill</a>.</dd></dl>
<h1><a class="anchor" id="sec8"></a>
(8) Open Source License</h1>
<dl class="section user"><dt></dt><dd>CUB is available under the "New BSD" open-source license:</dd></dl>
<dl class="section user"><dt></dt><dd><div class="fragment"><div class="line">Copyright (c) 2011, Duane Merrill.  All rights reserved.</div>
<div class="line">Copyright (c) 2011-2013, NVIDIA CORPORATION.  All rights reserved.</div>
<div class="line"></div>
<div class="line">Redistribution and use in source and binary forms, with or without</div>
<div class="line">modification, are permitted provided that the following conditions are met:</div>
<div class="line">    * Redistributions of source code must retain the above copyright</div>
<div class="line">      notice, <span class="keyword">this</span> list of conditions and the following disclaimer.</div>
<div class="line">    * Redistributions in binary form must reproduce the above copyright</div>
<div class="line">      notice, <span class="keyword">this</span> list of conditions and the following disclaimer in the</div>
<div class="line">      documentation and/or other materials provided with the distribution.</div>
<div class="line">    * Neither the name of the NVIDIA CORPORATION nor the</div>
<div class="line">      names of its contributors may be used to endorse or promote products</div>
<div class="line">      derived from <span class="keyword">this</span> software without specific prior written permission.</div>
<div class="line"></div>
<div class="line">THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS <span class="stringliteral">&quot;AS IS&quot;</span> AND</div>
<div class="line">ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED</div>
<div class="line">WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE</div>
<div class="line">DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY</div>
<div class="line">DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES</div>
<div class="line">(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;</div>
<div class="line">LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND</div>
<div class="line">ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT</div>
<div class="line">(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS</div>
<div class="line">SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.</div>
</div><!-- fragment --> </dd></dl>
</div></div><!-- contents -->
<!-- HTML footer for doxygen 1.8.3.1-->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated on Wed May 1 2013 05:38:35 for CUB by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.3.1
<br>
&copy; 2013 NVIDIA Corporation
</small></address>
</body>
</html>
