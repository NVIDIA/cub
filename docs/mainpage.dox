/******************************************************************************
 * Copyright (c) 2011, Duane Merrill.  All rights reserved.
 * Copyright (c) 2011-2013, NVIDIA CORPORATION.  All rights reserved.
 * 
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 * 
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 ******************************************************************************/
 
 

/**
 * \mainpage
 *
 * \tableofcontents
 *
 * \htmlonly
 * <a href="http://research.nvidia.com"><img src="nvresearch.png" style="position:relative; bottom:-10px; border:0px;"/></a>
 * &nbsp;&nbsp;
 * <a href="http://research.nvidia.com"><em>NVIDIA Research</em></a>
 * <br>
 * <a href="https://github.com/NVlabs/cub"><img src="github-icon-747d8b799a48162434b2c0595ba1317e.png" style="position:relative; bottom:-10px; border:0px;"/></a>
 * &nbsp;&nbsp;
 * <a href="https://github.com/NVlabs/cub"><em>Browse or fork CUB at GitHub!</em></a>
 * <br>
 * <a href="http://groups.google.com/group/cub-users"><img src="groups-icon.png" style="position:relative; bottom:-10px; border:0px;"/></a>
 * &nbsp;&nbsp;
 * <a href="http://groups.google.com/group/cub-users"><em>Join the cub-users discussion forum!</em></a>
 * <br>
 * <a href="download_cub.html"><img src="download-icon.png" style="position:relative; bottom:-10px; border:0px;"/></a>
 * &nbsp;&nbsp;
 * <a href="download_cub.html"><em>Download CUB!</em></a>
 * \endhtmlonly
 *
 * \section sec0 (1) What is CUB?
 *
 * \par
 * <b><em>Overview</em></b>. CUB is an open-source library of high performance 
 * primitives and utilities for constructing CUDA software. CUB provides reusable 
 * software components for every layer of execution within the CUDA programming 
 * model:
 * - [<b><em>Device-wide primitives</em></b>] (group___device_module.html) 
 *   - Global histogram, reduction, etc.  (More coming soon..)  
 *   - Compatible with CUDA dynamic/nested parallelism
 * - [<b><em>Block-wide primitives</em></b>] (group___block_module.html)
 *   - Local radix sort, prefix scan, histogram, reduction, I/O, etc.
 *   - Compatible with arbitrary thread block sizes 
 * - [<b><em>Warp-wide primitives</em></b>] (group___warp_module.html)
 *   - Local prefix scan, reduction, etc.
 * - [<b><em>Thread</em></b>](group___thread_module.html), [<b><em>grid dispatch</em></b>](group___grid_module.html), and [<b><em>resource utilities</em></b>] (group___thread_module.html)
 *   - PTX intrinsics, device reflection, work distribution, caching memory allocators, etc. 
 *
 * \par 
 * <b><em>Motivation</em></b>. CUB is inspired by the following goals:
 * - <em>Absolute performance</em>.  CUB primitives are specialized and tuned to 
 *   best match the features and capabilities of every CUDA architecture.
 * - <em>Enhanced programmer productivity</em>.  CUB primitives allow developers to quickly 
 *   string together sequences of complex parallel operations at all levels of execution.       
 * - <em>Reduced maintenance burden</em>.  CUB provides layers of abstraction over
 *   the complexities of diverse CUDA hardware.  With CUB, applications can 
 * 	 enjoy performance-portability without the intensive rewriting or porting efforts 
 *   typically needed to accommodate new device instructions or behavior.  
 * 
 * \par
 * <b><em>Collective primitives</em></b>. CUB is unique in that it provides cooperative block-level and 
 * warp-level parallel primitives.  Unlike traditional library software 
 * components, these operations are <em>collective</em>, i.e., they are called 
 * by a group of parallel SIMT threads rather than by a single sequential 
 * thread.  These collective primitives are not bound to any particular width 
 * of parallelism or to any particular data type.  This allows them to be 
 * flexible and tunable to fit the needs of the enclosing kernel computation.
 * \par
 * Thus CUB is [<em>CUDA Unbound</em>](index.html).
 *
 * \par
 * \image html cub_overview.png
 * <div class="centercaption">Orientation of <em>collective</em> CUB thread block primitives within the CUDA software stack</div>
 * 
 * \section sec2 (2) Recent news
 *
 * \par
 * <table>
 * <tr> <td>04/30/2013</td> <td style="white-space: nowrap">[CUB v0.9.3 (update release)](https://github.com/NVlabs/cub/archive/0.9.3.zip)</td> 	<td>Introduced several new device-wide and block-wide primitives, including 256-bin histogram.  Misc. cosmetic and bug fixes.  See the [change-log](https://github.com/NVlabs/cub/blob/master/CHANGE_LOG.TXT) for further details.</td> </tr>
 * <tr> <td>04/04/2013</td> <td style="white-space: nowrap">[CUB v0.9.2 (update release)](https://github.com/NVlabs/cub/archive/0.9.2.zip)</td> 	<td>Minor cosmetic, feature, and compilation updates.  See the [change-log](https://github.com/NVlabs/cub/blob/master/CHANGE_LOG.TXT) for further details.</td> </tr>
 * <tr> <td>03/07/2013</td> <td style="white-space: nowrap">[CUB v0.9 ("preview" release)](https://github.com/NVlabs/cub/archive/0.9.zip)</td> 		<td>CUB is the first durable, high-performance library of cooperative threadblock, warp, and thread primitives for CUDA kernel programming. More primitives and examples coming soon!</td> </tr>
 * </table> 
 *
 * \section sec3 (3) A simple example
 *
 * \par
 * The following code snippet illustrates a simple CUDA kernel for sorting a thread block's data:
 *
 * \par
 * \code
 * #include <cub/cub.cuh>
 *
 * // An tile-sorting CUDA kernel
 * template <
 *      int         BLOCK_THREADS,              // Threads per block
 *      int         ITEMS_PER_THREAD,           // Items per thread
 *      typename    T>                          // Numeric data type
 * __global__ void TileSortKernel(T *d_in, T *d_out)
 * {
 *      using namespace cub;
 *      const int TILE_SIZE = BLOCK_THREADS * ITEMS_PER_THREAD;
 *
 *      // Parameterize cub::BlockRadixSort for the parallel execution context
 *      typedef BlockRadixSort<T, BLOCK_THREADS> BlockRadixSort;
 *
 *      // Declare the shared memory needed by BlockRadixSort
 *      __shared__ typename BlockRadixSort::SmemStorage smem_storage;
 *
 *      // A segment of data items per thread
 *      T data[ITEMS_PER_THREAD];
 *
 *      // Load a tile of data using vector-load instructions
 *      BlockLoadVectorized(data, d_in + (blockIdx.x * TILE_SIZE));
 *
 *      // Sort data in ascending order
 *      BlockRadixSort::SortBlocked(smem_storage, data);
 *
 *      // Store the sorted tile using vector-store instructions
 *      BlockStoreVectorized(data, d_out + (blockIdx.x * TILE_SIZE));
 * }
 * \endcode
 *
 * \par
 * The cub::BlockRadixSort type performs a cooperative radix sort across the
 * thread block's data items.  Its implementation is parameterized by the number of threads per block and the aggregate
 * data type \p T and is specialized for the underlying architecture.
 *
 * \par
 * Once instantiated, the cub::BlockRadixSort type exposes an opaque cub::BlockRadixSort::SmemStorage
 * member type.  The thread block uses this storage type to allocate the shared memory needed by the
 * primitive.  This storage type can be aliased or <tt>union</tt>'d with other types so that the
 * shared memory can be reused for other purposes.
 *
 * \par
 * Furthermore, the kernel uses CUB's primitives for vectorizing global
 * loads and stores.  For example, lower-level <tt>ld.global.v4.s32</tt>
 * [PTX instructions](http://docs.nvidia.com/cuda/parallel-thread-execution)
 * will be generated when \p T = \p int and \p ITEMS_PER_THREAD is a multiple of 4.
 *
 * \section sec4 (4) Why do you need CUB?
 *
 * \par
 * CUDA kernel software is where the complexity of parallelism is expressed.
 * Programmers must reason about deadlock, livelock, synchronization, race conditions,
 * shared memory layout, plurality of state, granularity, throughput, latency,
 * memory bottlenecks, etc. Constructing and fine-tuning kernel code is perhaps the
 * most challenging, time-consuming aspect of CUDA programming.
 *
 * \par
 * However, with the exception of CUB, there are few (if any) software libraries of
 * reusable kernel primitives. In the CUDA ecosystem, CUB is unique in this regard.
 * As a [SIMT](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
 * library and software abstraction layer, CUB provides:
 * -# <b>Simplicity of composition.</b>  Parallel CUB primitives can be simply sequenced
 *    together in kernel code.  (This convenience is analogous to programming with
 *    [<b><em>Thrust</em></b>](http://thrust.github.com/) primitives in the host program.)
 * -# <b>High performance.</b> CUB simplifies high performance kernel development by
 *    taking care to implement and make available the fastest available algorithms,
 *    strategies, and techniques.
 * -# <b>Performance portability.</b> CUB primitives are specialized to match
 *    the target hardware.  Furthermore, the CUB library continually evolves to accommodate new
 *    algorithmic developments, hardware instructions, etc.
 * -# <b>Simplicity of performance tuning.</b>  CUB primitives provide parallel abstractions
 *    whose performance behavior can be statically tuned.  For example, most CUB primitives
 *    support alternative algorithmic strategies and variable grain sizes (threads per block,
 *    items per thread, etc.).
 * -# <b>Robustness and durability.</b> CUB primitives are designed to function properly for
 *    arbitrary data types and widths of parallelism (not just for the built-in C++ types
 *    or for powers-of-two threads per block).
 *
 * \section sec5 (5) Where is CUB positioned in the CUDA ecosystem?
 *
 * \par
 * The CUDA ecosystem engenders four different layers of software abstraction:
 *
 * <table border="0px" cellpadding="0px" cellspacing="0px"><tr>
 * <td width="50%">
 * \par
 * <b>Runtime-agnostic</b>.  A single thread calls through a library interface to 
 * perform some data-parallel function.  The use of any particular parallel 
 * or sequential computing backend (e.g., CUDA, TBB, OpenMP, etc.) is opaque to the caller.  
 * Although these libraries are not CUDA-specific, primitives such as those 
 * provided by CUB can be used to implement them.  Examples include:
 * - [<b><em>Thrust</em></b>](http://thrust.github.com/)
 * - [<b><em>Hemi</em></b>](https://github.com/harrism/hemi)
 * - [<b><em>ArrayFire</em></b>](http://www.accelereyes.com/products/arrayfire)
 * </td>
 * <td width="50%">
 * \htmlonly
 * <a href="generic_abstraction.png"><img src="generic_abstraction.png" width="100%" border="0px"/></a>
 * \endhtmlonly
 * </td>
 * </tr><tr>
 * <td width="50%">
 * \par
 * <b>CUDA kernel</b>.  A single CPU thread invokes a CUDA kernel to perform
 * some data-parallel function.  The incorporation of entire kernels (and their
 * corresponding invocation stubs) into libraries is the most common form of code reuse for
 * CUDA.  Libraries of CUDA kernels include the following:
 * - [<b><em>cuBLAS</em></b>](https://developer.nvidia.com/cublas)
 * - [<b><em>cuFF</em>T</b>](https://developer.nvidia.com/cufft)
 * - [<b><em>cuSPARSE</em></b>](https://developer.nvidia.com/cusparse)
 * - [<b><em>CUB</em></b>](index.html)
 * </td>
 * <td width="50%">
 * \htmlonly
 * <a href="kernel_abstraction.png"><img src="kernel_abstraction.png" width="100%" border="0px"/></a>
 * \endhtmlonly
 * </td>
 * </tr><tr>
 * <td>
 * \par
 * <b>Thread blocks (SIMT)</b>.  Each kernel invocation comprises some number of parallel
 * threads.  Threads are grouped into blocks, and the entire block of threads invokes some cooperative
 * function  in which they communicate and synchronize with each other.  There has historically been very
 * little reuse of cooperative SIMT software within CUDA kernel.  Libraries of thread-block primitives
 * include the following:
 * - [<b><em>CUB</em></b>](index.html)
 * </td>
 * <td>
 * \htmlonly
 * <a href="simt_abstraction.png"><img src="simt_abstraction.png" width="100%" border="0px"/></a>
 * \endhtmlonly
 * </td>
 * </tr><tr>
 * <td>
 * \par
 * <b>CUDA thread</b>.  A single CUDA thread invokes some sequential function.
 * This is the finest-grained level of CUDA software abstraction and requires
 * no consideration for the scheduling or synchronization of parallel threads.  CUDA libraries of
 * purely data-parallel functions include the following:
 * - [<b><em> CUDA Math</em></b>](http://docs.nvidia.com/cuda/cuda-math-api/index.html),
 *   [<b><em>Texture</em></b>](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#texture-functions), and
 *   [<b><em>Atomic</em></b>](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#atomic-functions) APIs
 * - [<b><em>cuRAND</em></b>](https://developer.nvidia.com/curand)'s device-code interface
 * - [<b><em>CUB</em></b>](index.html)
 * </td>
 * <td>
 * \htmlonly
 * <a href="devfun_abstraction.png"><img src="devfun_abstraction.png" width="100%" border="0px"/></a>
 * \endhtmlonly
 * </td>
 * </tr></table>
 *
 *
 * \section sec6 (6) How does CUB work?
 *
 * \par
 * CUB leverages the following programming idioms:
 * -# [<b>C++ templates</b>](index.html#sec3sec1)
 * -# [<b>Reflective type structure</b>](index.html#sec3sec2)
 * -# [<b>Flexible data mapping</b>](index.html#sec3sec3)
 *
 * \subsection sec3sec1 6.1 &nbsp;&nbsp; C++ templates
 *
 * \par
 * As a SIMT library, CUB must be flexible enough to accommodate a wide spectrum
 * of <em>parallel execution contexts</em>,
 * i.e., specific:
 *    - Data types
 *    - Widths of parallelism (threads per block)
 *    - Grain sizes (data items per thread)
 *    - Underlying architectures (special instructions, warp size, rules for bank conflicts, etc.)
 *    - Tuning requirements (e.g., latency vs. throughput)
 *
 * \par
 * To provide this flexibility, CUB is implemented as a C++ template library.
 * C++ templates are a way to write generic algorithms and data structures.
 * There is no need to build CUB separately.  You simply <tt>\#include</tt> the
 * <tt>cub.cuh</tt> header file into your <tt>.cu</tt> CUDA C++ sources
 * and compile with NVIDIA's <tt>nvcc</tt> compiler.
 *
 * \subsection sec3sec2 6.2 &nbsp;&nbsp; Reflective type structure
 *
 * \par
 * Cooperation within a thread block requires shared memory for communicating between threads.
 * However, the specific size and layout of the memory needed by a given
 * primitive will be specific to the details of its parallel execution context (e.g., how
 * many threads are calling into it, how many items are processed per thread, etc.).  Furthermore,
 * this shared memory must be allocated outside of the component itself if it is to be
 * reused elsewhere by the thread block.
 *
 * \par
 * \code
 * // Parameterize a BlockScan type for use with 128 threads
 * // and 4 items per thread
 * typedef cub::BlockScan<unsigned int, 128, 4> BlockScan;
 *
 * // Declare shared memory for BlockScan
 * __shared__ typename BlockScan::SmemStorage smem_storage;
 *
 * // A segment of consecutive input items per thread
 * int data[4];
 *
 * // Obtain data in blocked order
 * ...
 *
 * // Perform an exclusive prefix sum across the tile of data
 * BlockScan::ExclusiveSum(smem_storage, data, data);
 *
 * \endcode
 *
 * \par
 * To address this issue, we encapsulate cooperative procedures within
 * <em>reflective type structure</em> (C++ classes).  As illustrated in the
 * cub::BlockScan example above, these primitives are C++ classes with
 * interfaces that expose both:
 * - Procedural entrypoints for a block of threads to invoke
 * - An opaque shared memory type needed for the operation of those methods
 *
 * \subsection sec3sec3 6.3 &nbsp;&nbsp; Flexible mapping of data onto threads
 *
 * \par
 * We often design kernels such that each thread block is assigned a "tile" of data
 * items for processing.
 *
 * \par
 * \image html tile.png
 * <div class="centercaption">Tile of eight ordered data items</div>
 *
 * \par
 * When the tile size equals the thread block size, the
 * mapping of data onto threads is straightforward (one datum per thread).
 * However, there are often performance advantages for processing more
 * than one datum per thread.  For these scenarios, CUB primitives
 * will specify which of the following partitioning alternatives they 
 * accommodate:
 *
 * <table border="0px" cellpadding="0px" cellspacing="0px"><tr>
 * <td>
 * \par
 * - <b><em>Blocked arrangement</em></b>.  The aggregate tile of items is partitioned
 *   evenly across threads in "blocked" fashion with thread<sub><em>i</em></sub>
 *   owning the <em>i</em><sup>th</sup> segment of consecutive elements.
 *   Blocked arrangements are often desirable for algorithmic benefits (where
 *   long sequences of items can be processed sequentially within each thread).
 * </td>
 * <td>
 * \par
 * \image html blocked.png
 * <div class="centercaption"><em>Blocked</em> arrangement across four threads <br>(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>
 * </td>
 * </tr><tr>
 * <td>
 * \par
 * - <b><em>Striped arrangement</em></b>.  The aggregate tile of items is partitioned across
 *   threads in "striped" fashion, i.e., the \p ITEMS_PER_THREAD items owned by
 *   each thread have logical stride \p BLOCK_THREADS between them. Striped arrangements
 *   are often desirable for data movement through global memory (where
 *   [read/write coalescing](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-global-memory)</a>
 *   is an important performance consideration).
 * </td>
 * <td>
 * \par
 * \image html striped.png
 * <div class="centercaption"><em>Striped</em> arrangement across four threads <br>(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>
 * </td>
 * </tr></table>
 *
 * \par
 * The benefits of processing multiple items per thread (a.k.a., <em>register blocking</em>, <em>granularity coarsening</em>, etc.) include:
 * - Algorithmic efficiency.  Sequential work over multiple items in
 *   thread-private registers is cheaper than synchronized, cooperative
 *   work through shared memory spaces.
 * - Data occupancy.  The number of items that can be resident on-chip in
 *   thread-private register storage is often greater than the number of
 *   schedulable threads.
 * - Instruction-level parallelism.  Multiple items per thread also
 *   facilitates greater ILP for improved throughput and utilization.
 *
 * \par
 * Finally, cub::BlockExchange provides operations for converting between blocked
 * and striped arrangements.
 *
 * \section sec7 (7) Contributors
 *
 * \par
 * CUB is developed as an open-source project by [NVIDIA Research](http://research.nvidia.com).
 * The primary contributor is [Duane Merrill](http://github.com/dumerrill).
 *
 * \section sec8 (8) Open Source License
 *
 * \par
 * CUB is available under the "New BSD" open-source license:
 *
 * \par
 * \code
 * Copyright (c) 2011, Duane Merrill.  All rights reserved.
 * Copyright (c) 2011-2013, NVIDIA CORPORATION.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 * \endcode
 *
 */


/**
 * \defgroup DeviceModule Device-wide
 */

/**
 * \defgroup BlockModule Block-wide
 */

/**
 * \defgroup WarpModule Warp-wide
 */

/**
 * \defgroup ThreadModule Thread
 */

/**
 * \defgroup GridModule Grid utilities
 */

/**
 * \defgroup UtilModule Generic CUB utilities
 */
