/******************************************************************************
 * Copyright (c) 2011, Duane Merrill.  All rights reserved.
 * Copyright (c) 2011-2022, NVIDIA CORPORATION.  All rights reserved.
 * 
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *     * Redistributions of source code must retain the above copyright
 *       notice, this list of conditions and the following disclaimer.
 *     * Redistributions in binary form must reproduce the above copyright
 *       notice, this list of conditions and the following disclaimer in the
 *       documentation and/or other materials provided with the distribution.
 *     * Neither the name of the NVIDIA CORPORATION nor the
 *       names of its contributors may be used to endorse or promote products
 *       derived from this software without specific prior written permission.
 * 
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
 * WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL NVIDIA CORPORATION BE LIABLE FOR ANY
 * DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
 * (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
 * LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND
 * ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
 * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 ******************************************************************************/
 
 

/**

\defgroup CollectiveModule     SIMT "collective" primitives
    \defgroup WarpModule             Warp-wide
        \ingroup CollectiveModule
    \defgroup BlockModule             Block-wide
        \ingroup CollectiveModule

\defgroup DeviceModule        Device-wide primitives
    \defgroup SingleModule             Single-problem
        \ingroup DeviceModule
    \defgroup SegmentedModule         Segmented-problem (batch)
        \ingroup DeviceModule

\defgroup UtilModule     Utilities
    \defgroup UtilIterator     Fancy iterators
        \ingroup UtilModule
    \defgroup UtilIo         Thread and thread block I/O
        \ingroup UtilModule
    \defgroup UtilPtx         PTX intrinsics
        \ingroup UtilModule
    \defgroup UtilMgmt      Device, kernel, and storage management
        \ingroup UtilModule

\mainpage

\tableofcontents

\htmlonly

<table border="0px" cellpadding="0px" cellspacing="0px"><tr><td>

<a href="download_cub.html"><img src="download-icon.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="download_cub.html"><em><b>Download CUB <span <span class="projectnumber">v</span> </b></em></a>

</td><td>

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<a href="http://research.nvidia.com"><img src="nvresearch.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="http://research.nvidia.com"><em>NVIDIA Research</em></a>

</td></tr><tr><td>

<a href="https://github.com/NVIDIA/cub"><img src="github-icon-747d8b799a48162434b2c0595ba1317e.png" style="position:relative; bottom:-10px; border:0px;"/></a>
&nbsp;&nbsp;
<a href="https://github.com/NVIDIA/cub"><em>Browse or fork CUB at GitHub</em></a>

</td><td>
</td></tr></table>

\endhtmlonly

\section sec1 (1) What is CUB?

\par
CUB provides state-of-the-art, reusable software components for every layer 
of the CUDA programming model:
- <b><em>Parallel primitives</em></b>
  - [<b><em>Warp-wide "collective" primitives</em></b>] (group___warp_module.html)
    - Cooperative warp-wide prefix scan, reduction, etc.
    - Safely specialized for each underlying CUDA architecture
  - [<b><em>Block-wide "collective" primitives</em></b>] (group___block_module.html)
    - Cooperative I/O, sort, scan, reduction, histogram, etc.  
    - Compatible with arbitrary thread block sizes and types 
  - [<b><em>Device-wide primitives</em></b>] (group___device_module.html) 
    - Parallel sort, prefix scan, reduction, histogram, etc.  
    - Compatible with CUDA dynamic parallelism
- <b><em>Utilities</em></b>
  - [<b><em>Fancy iterators</em></b>] (group___util_iterator.html)
  - [<b><em>Thread and thread block I/O</em></b>] (group___util_io.html)
  - [<b><em>PTX intrinsics</em></b>] (group___util_ptx.html)
  - [<b><em>Device, kernel, and storage management</em></b>] (group___util_mgmt.html)

\section sec2 (2) CUB's collective primitives 
\par
Collective software primitives are essential for constructing high-performance, 
maintainable CUDA kernel code.  Collectives allow complex parallel code to be 
re-used rather than re-implemented, and to be re-compiled rather than 
hand-ported. <br><br>
 
\par
\image html cub_overview.png
<div class="centercaption">Orientation of <em>collective</em> primitives within the CUDA software stack</div>

\par
As a SIMT programming model, CUDA engenders both <em><b>scalar</b></em> and 
<em><b>collective</b></em> software interfaces. Traditional software 
interfaces are <em>scalar</em> : a single thread invokes a library routine to perform some 
operation (which may include spawning parallel subtasks).  Alternatively, a <em>collective</em> 
interface is entered simultaneously by a group of parallel threads to perform 
some cooperative operation.   

\par
CUB's collective primitives are not bound to any particular width of parallelism 
or data type.  This flexibility makes them:
- <b><em>Adaptable</em></b> to fit the needs of the enclosing kernel computation
- <b><em>Trivially tunable</em></b> to different grain sizes (threads per block, 
  items per thread, etc.)

\par
Thus CUB is [<em>CUDA Unbound</em>](index.html).

\section sec3 (3) An example (block-wide sorting)

\par
The following code snippet presents a CUDA kernel in which each block of \p BLOCK_THREADS threads 
will collectively load, sort, and store its own segment of (\p BLOCK_THREADS * \p ITEMS_PER_THREAD) 
integer keys:

\par
\code
#include <cub/cub.cuh>

//
// Block-sorting CUDA kernel
//
template <int BLOCK_THREADS, int ITEMS_PER_THREAD>
__global__ void BlockSortKernel(int *d_in, int *d_out)
{
    // Specialize BlockLoad, BlockStore, and BlockRadixSort collective types
    typedef cub::BlockLoad<
        int*, BLOCK_THREADS, ITEMS_PER_THREAD, BLOCK_LOAD_TRANSPOSE> BlockLoadT;
    typedef cub::BlockStore<
        int*, BLOCK_THREADS, ITEMS_PER_THREAD, BLOCK_STORE_TRANSPOSE> BlockStoreT;
    typedef cub::BlockRadixSort<
        int, BLOCK_THREADS, ITEMS_PER_THREAD> BlockRadixSortT;

    // Allocate type-safe, repurposable shared memory for collectives
    __shared__ union {
        typename BlockLoadT::TempStorage       load; 
        typename BlockStoreT::TempStorage      store; 
        typename BlockRadixSortT::TempStorage  sort;
    } temp_storage; 

    // Obtain this block's segment of consecutive keys (blocked across threads)
    int thread_keys[ITEMS_PER_THREAD];
    int block_offset = blockIdx.x * (BLOCK_THREADS * ITEMS_PER_THREAD);      
    BlockLoadT(temp_storage.load).Load(d_in + block_offset, thread_keys);
    
    __syncthreads();    // Barrier for smem reuse

    // Collectively sort the keys
    BlockRadixSortT(temp_storage.sort).Sort(thread_keys);

    __syncthreads();    // Barrier for smem reuse

    // Store the sorted segment 
    BlockStoreT(temp_storage.store).Store(d_out + block_offset, thread_keys);
}
\endcode

\par 
\code

// Elsewhere in the host program: parameterize and launch a block-sorting 
// kernel in which blocks of 128 threads each sort segments of 2048 keys
int *d_in = ...; 
int *d_out = ...;
int num_blocks = ...;
BlockSortKernel<128, 16><<<num_blocks, 128>>>(d_in, d_out); 

\endcode

\par
In this example, threads use cub::BlockLoad, cub::BlockRadixSort, and cub::BlockStore to collectively
load, sort and store the block's segment of input items.  Because these operations are 
cooperative, each primitive requires an allocation of shared memory for threads to communicate 
through.  The typical usage pattern for a CUB collective is:
-# Statically specialize the primitive for the specific problem setting at hand, e.g.,
   the data type being sorted, the number of threads per block, the number of keys per 
   thread, optional algorithmic alternatives, etc.  (CUB primitives are also implicitly 
   specialized by the targeted compilation architecture.)
-# Allocate (or alias) an instance of the specialized primitive's nested \p TempStorage 
   type within a shared memory space.
-# Specify communication details (e.g., the \p TempStorage allocation) to 
   construct an instance of the primitive.
-# Invoke methods on the primitive instance.

\par
In particular, cub::BlockRadixSort is used to collectively sort the segment of data items 
that have been partitioned across the thread block. To provide coalesced accesses 
to device memory,  we configure the cub::BlockLoad and cub::BlockStore primitives 
to access memory using a striped access pattern (where consecutive threads 
simultaneously access consecutive items) and then <em>transpose</em> the keys into 
a [<em>blocked arrangement</em>](index.html#sec4sec3) of elements across threads.  
To reuse shared memory across all three primitives, the thread block statically 
allocates a union of their \p TempStorage types.

\section sec4 (4) Why do you need CUB?

\par
Writing, tuning, and maintaining kernel code is perhaps the most challenging, 
time-consuming aspect of CUDA programming.  Kernel software is where 
the complexity of parallelism is expressed. Programmers must reason about 
deadlock, livelock, synchronization, race conditions, shared memory layout, 
plurality of state, granularity, throughput, latency, memory bottlenecks, etc. 

\par
With the exception of CUB, however, there are few (if any) software libraries of
<em>reusable</em> kernel primitives. In the CUDA ecosystem, CUB is unique in this regard.
As a [SIMT](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation)
library and software abstraction layer, CUB provides:

-# <b><em>Simplicity of composition</em></b>. CUB enhances programmer productivity by 
   allowing complex parallel operations to be easily sequenced and nested.  
   For example, cub::BlockRadixSort is constructed from cub::BlockExchange and 
   cub::BlockRadixRank. The latter is composed of cub::BlockScan 
   which incorporates cub::WarpScan. \image html nested_composition.png
   
-# <b><em>High performance</em></b>. CUB simplifies high-performance program and kernel 
   development by taking care to implement the state-of-the-art in parallel algorithms.<br><br>  
    
-# <b><em>Performance portability</em></b>. 
   CUB primitives are specialized to match the diversity of NVIDIA hardware, continuously 
   evolving to accommodate new architecture-specific features and instructions.  And 
   because CUB's device-wide primitives are implemented using flexible block-wide and 
   warp-wide collectives, we are able to performance-tune them to match the processor 
   resources provided by each CUDA processor architecture. As a result, our CUB 
   implementations demonstrate much better performance-portability when compared to more 
   traditional, rigidly-coded parallel libraries such 
   as [<b><em>Thrust</em></b>](http://thrust.github.io/): <br><br><img src="scan_int32.png"><br><br> 
   
-# <b><em>Simplicity of performance tuning</em></b>:
  - <b><em>Resource utilization</em></b>.  CUB primitives allow developers to quickly 
    change grain sizes (threads per block, items per thread, etc.) to best match
    the processor resources of their target architecture
  - <b><em>Variant tuning</em></b>.  Most CUB primitives support alternative algorithmic 
    strategies. For example, cub::BlockHistogram is parameterized to implement either 
    an atomic-based approach or a sorting-based approach.  (The latter provides uniform 
    performance regardless of input distribution.)
  - <b><em>Co-optimization</em></b>.  When the enclosing kernel 
    is similarly parameterizable, a tuning configuration can be found that optimally 
    accommodates their combined register and shared memory pressure.<br><br>
     
-# <b><em>Robustness and durability</em></b>. CUB just works.  CUB primitives 
   are designed to function properly for arbitrary data types and widths of 
   parallelism (not just for the built-in C++ types or for powers-of-two threads 
   per block).<br><br>
   
-# <em><b>Reduced maintenance burden</b></em>.  CUB provides a SIMT software abstraction layer 
  over the diversity of CUDA hardware.  With CUB, applications can enjoy 
  performance-portability without intensive and costly rewriting or porting efforts.  <br><br>

-# <b><em>A path for language evolution</em></b>.  CUB primitives are designed 
   to easily accommodate new features in the CUDA programming model, e.g., thread 
   subgroups and named barriers, dynamic shared memory allocators, etc. <br><br>

\section sec5 (5) How do CUB collectives work?
 
\par
Four programming idioms are central to the design of CUB: 
-# [<b><em>Generic programming</em></b>](index.html#sec4sec1).  C++ templates provide the flexibility 
   and adaptive code generation needed for CUB primitives to be useful, reusable, and  
   fast in arbitrary kernel settings.
-# [<b><em>Reflective class interfaces</em></b>](index.html#sec4sec2).  CUB collectives statically export their 
   their resource requirements (e.g., shared memory size and layout) for a 
   given specialization, which allows compile-time tuning decisions and resource 
   allocation.
-# [<b><em>Flexible data arrangement across threads</em></b>](index.html#sec4sec3).  CUB collectives operate on 
   data that is logically partitioned across a group of threads.  For most collective
   operations, efficiency is increased with increased granularity (i.e., items per thread).
-# [<b><em>Static tuning and co-tuning</em></b>](index.html#sec4sec4).  Simple constants and
   static types dictate the granularities and algorithmic alternatives to be employed by 
   CUB collectives.  When the enclosing kernel is similarly parameterized, an optimal 
   configuration can be determined that best accommodates the combined behavior and 
   resource consumption of all primitives within the kernel.

\subsection sec5sec1 5.1 Generic programming
\par
We use template parameters to specialize CUB primitives for the particular 
problem setting at hand.  Until compile time, CUB primitives are not bound 
to any particular:
- Data type (int, float, double, etc.)
- Width of parallelism (threads per thread block)
- Grain size (data items per thread)
- Underlying processor (special instructions, warp size, rules for bank conflicts, etc.)
- Tuning configuration (e.g., latency vs. throughput, algorithm selection, etc.)

\subsection sec5sec2 5.2 Reflective class interfaces 
\par
Unlike traditional function-oriented interfaces, CUB exposes its collective 
primitives as templated C++ classes.  The resource requirements for a specific 
parameterization are reflectively advertised as members of the class.  The 
resources can then be statically or dynamically allocated, aliased
to global or shared memory, etc.   The following illustrates a CUDA kernel 
fragment performing a collective prefix sum across the threads of a thread block:

\par
\code
#include <cub/cub.cuh>

__global__ void SomeKernelFoo(...)
{
    // Specialize BlockScan for 128 threads on integer types
    typedef cub::BlockScan<int, 128> BlockScan;
 
    // Allocate shared memory for BlockScan
    __shared__ typename BlockScan::TempStorage scan_storage;

    ...
    
    // Obtain a segment of consecutive items that are blocked across threads
    int thread_data_in[4];
    int thread_data_out[4];
    ...

    // Perform an exclusive block-wide prefix sum
    BlockScan(scan_storage).ExclusiveSum(thread_data_in, thread_data_out);

\endcode

\par
Furthermore, the CUB interface is designed to separate parameter 
fields by concerns.  CUB primitives have three distinct parameter fields:
-# <b><em>Static template parameters</em></b>.  These are constants that will 
   dictate the storage layout and the unrolling of algorithmic steps (e.g., 
   the input data type and the number of block threads), and are used to specialize the class.
-# <b><em>Constructor parameters</em></b>.  These are optional parameters regarding 
   inter-thread communication (e.g., storage allocation, thread-identifier mapping, 
   named barriers, etc.), and are orthogonal to the functions exposed by the class.
-# <b><em>Formal method parameters</em></b>. These are the operational inputs/outputs
   for the various functions exposed by the class.

\par
This allows CUB types to easily accommodate new 
programming model features (e.g., named barriers, memory allocators, etc.) 
without incurring a combinatorial growth of interface methods.   

\subsection sec5sec3 5.3 Flexible data arrangement across threads
\par
CUDA kernels are often designed such that each thread block is assigned a 
segment of data items for processing.

\par
\image html tile.png
<div class="centercaption">Segment of eight ordered data items</div>

\par
When the tile size equals the thread block size, the
mapping of data onto threads is straightforward (one datum per thread).
However, there are often performance advantages for processing more
than one datum per thread.  Increased granularity corresponds to 
decreased communication overhead.  For these scenarios, CUB primitives
will specify which of the following partitioning alternatives they 
accommodate:

<table border="0px" cellpadding="0px" cellspacing="0px"><tr>
<td>
\par
- <b><em>Blocked arrangement</em></b>.  The aggregate tile of items is partitioned
  evenly across threads in "blocked" fashion with thread<sub><em>i</em></sub>
  owning the <em>i</em><sup>th</sup> segment of consecutive elements.
  Blocked arrangements are often desirable for algorithmic benefits (where
  long sequences of items can be processed sequentially within each thread).
</td>
<td>
\par
\image html blocked.png
<div class="centercaption"><em>Blocked</em> arrangement across four threads <br>(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>
</td>
</tr><tr>
<td>
\par
- <b><em>Striped arrangement</em></b>.  The aggregate tile of items is partitioned across
  threads in "striped" fashion, i.e., the \p ITEMS_PER_THREAD items owned by
  each thread have logical stride \p BLOCK_THREADS between them. Striped arrangements
  are often desirable for data movement through global memory (where
  [read/write coalescing](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/#coalesced-access-global-memory)</a>
  is an important performance consideration).
</td>
<td>
\par
\image html striped.png
<div class="centercaption"><em>Striped</em> arrangement across four threads <br>(emphasis on items owned by <em>thread</em><sub>0</sub>)</div>
</td>
</tr></table>

\par
The benefits of processing multiple items per thread (a.k.a., <em>register blocking</em>, <em>granularity coarsening</em>, etc.) include:
- Algorithmic efficiency.  Sequential work over multiple items in
  thread-private registers is cheaper than synchronized, cooperative
  work through shared memory spaces.
- Data occupancy.  The number of items that can be resident on-chip in
  thread-private register storage is often greater than the number of
  schedulable threads.
- Instruction-level parallelism.  Multiple items per thread also
  facilitates greater ILP for improved throughput and utilization.

\par
Finally, cub::BlockExchange provides operations for converting between blocked
and striped arrangements.

\subsection sec5sec4 5.4 Static tuning and co-tuning
\par 
This style of flexible interface simplifies performance tuning.  Most CUB
primitives support alternative algorithmic strategies that can be
statically targeted by a compiler-based or JIT-based autotuner.  (For
example, cub::BlockHistogram is parameterized to implement either an
atomic-based approach or a sorting-based approach.)  Algorithms are also
tunable over parameters such as thread count and grain size as well.
Taken together, each of the CUB algorithms provides a fairly rich tuning
space.

\par
Whereas conventional libraries are optimized offline and in isolation, CUB 
provides interesting opportunities for whole-program optimization.  For 
example, each CUB primitive is typically parameterized by threads-per-block 
and items-per-thread, both of which affect the underlying algorithm's 
efficiency and resource requirements.  When the enclosing kernel is similarly 
parameterized, the coupled CUB primitives adjust accordingly.  This enables  
autotuners to search for a single configuration that maximizes the performance 
of the entire kernel for a given set of hardware resources.  
 
\section sec6 (6) How do I get started using CUB?

\par 
CUB is implemented as a C++ header library. There is no need to build CUB 
separately. To use CUB primitives in your code, simply:
-# [Download] (download_cub.html) and unzip the latest CUB distribution
-# <tt>#%include</tt> the "umbrella" <tt><cub/cub.cuh></tt> header file in 
   your CUDA C++ sources.  (Or <tt>#%include</tt> the particular 
   header files that define the CUB primitives you wish to use.)
-# Compile your program with NVIDIA's <tt>nvcc</tt> CUDA compiler, 
   specifying a \p -I<path-to-CUB> include-path flag to reference 
   the location of the CUB header library.

\par
We also have collection of simple [<b>CUB example programs</b>] (examples.html)

\section sec7 (7) How is CUB different than Thrust and Modern GPU?

\par CUB and Thrust
CUB and [<b><em>Thrust</em></b>](http://thrust.github.io/) share some 
similarities in that they both provide similar device-wide primitives for CUDA.  
However, they target different abstraction layers for parallel computing.
Thrust abstractions are agnostic of any particular parallel framework (e.g., 
CUDA, TBB, OpenMP, sequential  CPU, etc.).  While Thrust has a "backend" 
for CUDA devices, Thrust interfaces themselves are not CUDA-specific and 
do not explicitly expose CUDA-specific details (e.g., \p cudaStream_t parameters).
  
\par
CUB, on the other hand, is slightly lower-level than Thrust.  CUB is specific 
to CUDA C++ and its interfaces explicitly accommodate CUDA-specific features.  
Furthermore, CUB is also a library of SIMT collective primitives for block-wide 
and warp-wide kernel programming.

\par
CUB and Thrust are complementary and can be used together.  In fact, the CUB 
project arose out of a maintenance need to achieve better performance-portability 
within Thrust by using reusable block-wide primitives to reduce maintenance and 
tuning effort.  

\par CUB and Modern GPU
CUB and [<b><em>Modern GPU</em></b>](https://github.com/moderngpu/moderngpu) also
share some similarities in that they both implement similar device-wide primitives for CUDA.
However, they serve different purposes for the CUDA programming community.  MGPU
is a pedagogical tool for high-performance GPU computing, providing clear and concise 
exemplary code and accompanying commentary.  It serves as an excellent source of 
educational, tutorial, CUDA-by-example material.  The MGPU source code is intended 
to be read and studied, and often favors simplicity at the expense of portability and 
flexibility.

\par 
CUB, on the other hand, is a production-quality library whose sources are complicated 
by support for every version of CUDA architecture, and is validated by an extensive 
suite of regression tests.  Although well-documented, the CUB source text is verbose 
and relies heavily on C++ template metaprogramming for situational specialization.  

\par 
CUB and MGPU are complementary in that MGPU serves as an excellent descriptive source 
for many of the algorithmic techniques used by CUB. 

\section sec8 (8) Stable releases

\par
CUB releases are labeled using version identifiers having three fields: 
<tt>\<epoch\>.\<feature\>.\<update\></tt>.  The <em>epoch</em> field 
corresponds to support for a major change or update to the CUDA programming model.  
The <em>feature</em> field corresponds to a stable set of features, 
functionality, and interface.  The <em>update</em> field corresponds to a 
bug-fix or performance update for that feature set.  At the moment, we do 
not publicly provide non-stable releases such as development snapshots, 
beta releases or rolling releases.  (Feel free to contact us if you would 
like access to such things.)

\par
See the [releases](https://github.com/NVIDIA/cub/releases) page for latest releases and [change-log](https://github.com/NVIDIA/cub/blob/main/CHANGELOG.md) for summary and changes of each release.

\section sec9 (9) Contributors

\par
CUB is developed as an open-source project by [NVIDIA Research](http://research.nvidia.com).
The primary contributor is [Duane Merrill](http://github.com/dumerrill).
More information on configuring your CUB build and creating a pull request is found in [CONTRIBUTING.md](https://github.com/NVIDIA/cub/blob/main/CONTRIBUTING.md).

\section sec10 (10) Open Source License

\par
CUB is available under the [BSD 3-Clause "New" or "Revised" License](https://github.com/NVIDIA/cub/blob/main/LICENSE.TXT)



*/
